{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# For sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aita_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the features from https://www.aclweb.org/anthology/W18-1110.pdf\n",
    "\n",
    "# In general, every function starting with 'get' is applied to the data frame, the others are helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average sentence length as a feature\n",
    "def calcSentenceLength(text):\n",
    "    sent_text = nltk.sent_tokenize(text)\n",
    "    total_len = 0\n",
    "    num_sent = len(sent_text)\n",
    "    for s in sent_text:\n",
    "        tokenized = nltk.word_tokenize(s)\n",
    "        total_len = total_len + len(tokenized)\n",
    "    return total_len/num_sent\n",
    "\n",
    "def getSentLength(df):\n",
    "    df['sentence_length'] = df['text'].apply(lambda text: calcSentenceLength(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation as a feature\n",
    "def calcNegationCount(text):\n",
    "    text = text.split()\n",
    "    negation = re.compile(r\"\"\"(?:^(?:no|not)$)|n't\"\"\", re.verbose)\n",
    "    count = 0\n",
    "    for w in text:\n",
    "        if negation.search(w):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def getNegationCount(df):\n",
    "    df['negations'] = df['text'].apply(lambda text: calcNegationCount(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question words as a feature - as per Durmus et al\n",
    "# “why”,“when”,“how”,“what”,“who”,“whose”,“whom”,“where”, “whose”,“whether”\n",
    "# Not 100% sure this will work\n",
    "def getQuestionWords(df):\n",
    "    df['question_words'] = df['text'].apply(lambda text: len(re.findall(r'(where|why|when|how|what|who|whose|whom|whose|whether)', text)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag types\n",
    "# Proper nouns\n",
    "# Cardinals\n",
    "# Existential there\n",
    "# Personal pronouns - this was listed separately in the paper\n",
    "\n",
    "# list of tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "# Get POS given some text\n",
    "def addPosTags(text):\n",
    "    text = word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    return pos\n",
    "\n",
    "# Counts parts of speech in text\n",
    "def findPOSCount(text, pos_strings):\n",
    "    pos = addPosTags(text)\n",
    "    count = 0\n",
    "    for w in pos:\n",
    "        # Check the tag to see what it is\n",
    "        if w[1] is in pos_strings:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def getPOSCounts(df):\n",
    "    # Different tags to search for\n",
    "    proper_nouns = ['NNP', 'NNPS']\n",
    "    cardinals = ['CD']\n",
    "    exist_there = ['EX']\n",
    "    pers_pronouns = ['PRP']\n",
    "    # Adding counts of tags to database\n",
    "    df['proper_nouns'] = df['text'].apply(lambda text: findPosCount(text, proper_nouns))\n",
    "    df['cardinals'] = df['text'].apply(lambda text: findPosCount(text, cardinals))\n",
    "    df['existential_there'] = df['text'].apply(lambda text: findPosCount(text, exist_there))\n",
    "    df['personal_pronouns'] = df['text'].apply(lambda text: findPosCount(text, pers_pronouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative,positive and ambiguous emotions.\n",
    "def getSentiment(df):\n",
    "    # Dictionary with all scores\n",
    "    df['scores'] = df['text'].apply(lambda text: sid.polarity_scores(text))\n",
    "    df['compound'] = df['scores'].apply(lambda score_dict: score_dict['compound']) # Compound score\n",
    "    df['pos_neg'] = df['compound'].apply(lambda c: 'pos' if c >= 0 else 'neg') # Add pos/neg labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity mentions\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
