{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# For sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Parts of speech\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aita_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['text'].isna()])\n",
    "# 87 have 'NA' in the 'text' variable\n",
    "\n",
    "# Have to drop them for it to run successfully\n",
    "df = df.dropna(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the features from https://www.aclweb.org/anthology/W18-1110.pdf\n",
    "\n",
    "# In general, every function starting with 'get' is applied to the data frame, the others are helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average sentence length as a feature\n",
    "def calcSentenceLength(text):\n",
    "    sent_text = nltk.sent_tokenize(text)\n",
    "    total_len = 0\n",
    "    num_sent = len(sent_text)\n",
    "    for s in sent_text:\n",
    "        tokenized = nltk.word_tokenize(s)\n",
    "        total_len = total_len + len(tokenized)\n",
    "    return total_len/num_sent\n",
    "\n",
    "def getSentLength(df):\n",
    "    df['sentence_length'] = df['text'].apply(lambda text: calcSentenceLength(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation as a feature\n",
    "def calcNegationCount(text):\n",
    "    text = text.split()\n",
    "    negation = re.compile(r\"\"\"(?:^(?:no|not)$)|n't\"\"\")\n",
    "    count = 0\n",
    "    for w in text:\n",
    "        if negation.search(w):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def getNegationCount(df):\n",
    "    df['negations'] = df['text'].apply(lambda text: calcNegationCount(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question words as a feature - as per Durmus et al\n",
    "# “why”,“when”,“how”,“what”,“who”,“whose”,“whom”,“where”, “whose”,“whether”\n",
    "# Not 100% sure this will work\n",
    "def getQuestionWords(df):\n",
    "    df['question_words'] = df['text'].apply(lambda text: len(re.findall(r'(where|why|when|how|what|who|whose|whom|whose|whether)', text)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag types\n",
    "# Proper nouns\n",
    "# Cardinals\n",
    "# Existential there\n",
    "# Personal pronouns - this was listed separately in the paper\n",
    "\n",
    "# list of tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "# Get POS given some text\n",
    "def addPosTags(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    return pos\n",
    "\n",
    "# Counts parts of speech in text\n",
    "def findPOSCount(text, pos_strings):\n",
    "    pos = addPosTags(text)\n",
    "    count = 0\n",
    "    for w in pos:\n",
    "        # Check the tag to see what it is\n",
    "        if w[1] in pos_strings:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def getPOSCounts(df):\n",
    "    # Different tags to search for\n",
    "    proper_nouns = ['NNP', 'NNPS']\n",
    "    cardinals = ['CD']\n",
    "    exist_there = ['EX']\n",
    "    pers_pronouns = ['PRP']\n",
    "    # Adding counts of tags to database\n",
    "    df['proper_nouns'] = df['text'].apply(lambda text: findPOSCount(text, proper_nouns))\n",
    "    df['cardinals'] = df['text'].apply(lambda text: findPOSCount(text, cardinals))\n",
    "    df['existential_there'] = df['text'].apply(lambda text: findPOSCount(text, exist_there))\n",
    "    df['personal_pronouns'] = df['text'].apply(lambda text: findPOSCount(text, pers_pronouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative,positive and ambiguous emotions.\n",
    "def getSentiment(df):\n",
    "    # Dictionary with all scores\n",
    "    print(\"Getting sentiment scores...\")\n",
    "    df['scores'] = df['text'].apply(lambda text: sid.polarity_scores(text))\n",
    "    print(\"Getting compound scores...\")\n",
    "    df['compound'] = df['scores'].apply(lambda score_dict: score_dict['compound']) # Compound score\n",
    "    print(\"Getting positive/negative labels...\")\n",
    "    df['pos_neg'] = df['compound'].apply(lambda c: 'pos' if c >= 0 else 'neg') # Add pos/neg labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Extracting Sentiment\n",
      "Getting sentiment scores...\n",
      "Getting compound scores...\n",
      "Getting positive/negative labels...\n",
      "Extracting POS counts\n",
      "Extracting Question words\n",
      "Extracting Negations\n",
      "Extracting average sentence length\n"
     ]
    }
   ],
   "source": [
    "# Extract the features\n",
    "print(type(df))\n",
    "print(\"Extracting Sentiment\")\n",
    "getSentiment(df)\n",
    "print(\"Extracting POS counts\")\n",
    "getPOSCounts(df)\n",
    "print(\"Extracting Question words\")\n",
    "getQuestionWords(df)\n",
    "print(\"Extracting Negations\")\n",
    "getNegationCount(df)\n",
    "print(\"Extracting average sentence length\")\n",
    "getSentLength(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                                           1\n",
      "id                                                              1yu29c\n",
      "timestamp                                                  1.39328e+09\n",
      "title                             [AITA] Threw my parent's donuts away\n",
      "body                 My parents are diabetic, morbidly obese, and a...\n",
      "edited                                                    1393290576.0\n",
      "verdict                                                        asshole\n",
      "score                                                              140\n",
      "num_comments                                                        27\n",
      "is_asshole                                                           1\n",
      "text                 [AITA] Threw my parent's donuts away My parent...\n",
      "age                                                                NaN\n",
      "gender                                                             NaN\n",
      "scores               {'neg': 0.05, 'neu': 0.861, 'pos': 0.089, 'com...\n",
      "compound                                                        0.8591\n",
      "pos_neg                                                            pos\n",
      "proper_nouns                                                         5\n",
      "cardinals                                                            4\n",
      "existential_there                                                    1\n",
      "personal_pronouns                                                   19\n",
      "question_words                                                       1\n",
      "negations                                                            0\n",
      "sentence_length                                                15.0833\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new df with all the features added\n",
    "df.to_csv(\"aita_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
